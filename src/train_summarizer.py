# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBBEoXG8p8q1wekOUYe59wF2K4xVHWZ_
"""

import numpy as np
from time import time
import spacy
nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])
import re
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import pickle
import os

############### TRAINING ARGS
samples = 25000
latent_dim = 100
embedding_dim = 250
model_path = '../models/big_test'
###############

# Remove non-alphabetic characters (Data Cleaning)
def text_strip(column):

    for row in column:
        row = re.sub("(\\t)", " ", str(row)).lower()
        row = re.sub("(\\r)", " ", str(row)).lower()
        row = re.sub("(\\n)", " ", str(row)).lower()

        # Remove _ if it occurs more than one time consecutively
        row = re.sub("(__+)", " ", str(row)).lower()

        # Remove - if it occurs more than one time consecutively
        row = re.sub("(--+)", " ", str(row)).lower()

        # Remove ~ if it occurs more than one time consecutively
        row = re.sub("(~~+)", " ", str(row)).lower()

        # Remove + if it occurs more than one time consecutively
        row = re.sub("(\+\++)", " ", str(row)).lower()

        # Remove . if it occurs more than one time consecutively
        row = re.sub("(\.\.+)", " ", str(row)).lower()

        # Remove the characters - <>()|&©ø"',;?~*!
        row = re.sub(r"[<>()|&©ø\[\]\'\",;?~*!]", " ", str(row)).lower()

        # Remove mailto:
        row = re.sub("(mailto:)", " ", str(row)).lower()

        # Remove \x9* in text
        row = re.sub(r"(\\x9\d)", " ", str(row)).lower()

        # Replace INC nums to INC_NUM
        row = re.sub("([iI][nN][cC]\d+)", "INC_NUM", str(row)).lower()

        # Replace CM# and CHG# to CM_NUM
        row = re.sub("([cC][mM]\d+)|([cC][hH][gG]\d+)", "CM_NUM", str(row)).lower()

        # Remove punctuations at the end of a word
        row = re.sub("(\.\s+)", " ", str(row)).lower()
        row = re.sub("(\-\s+)", " ", str(row)).lower()
        row = re.sub("(\:\s+)", " ", str(row)).lower()

        # Replace any url to only the domain name
        try:
            url = re.search(r"((https*:\/*)([^\/\s]+))(.[^\s]+)", str(row))
            repl_url = url.group(3)
            row = re.sub(r"((https*:\/*)([^\/\s]+))(.[^\s]+)", repl_url, str(row))
        except:
            pass

        # Remove multiple spaces
        row = re.sub("(\s+)", " ", str(row)).lower()

        # Remove the single character hanging between any two spaces
        row = re.sub("(\s+.\s+)", " ", str(row)).lower()
        yield row


def prepare_data(pre, save_path='../data/pre.csv'):
    """
    Given a dataframe with text and summary, it returns the preprocessed dataframe
    """
    # Preprocess text
    processed_text = text_strip(pre['text'])
    processed_summary = text_strip(pre['summary'])
    print('Text Stripped')

    # Process text as batches and yield Doc objects in order
    text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=1000)]
    summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=5000)]
    print('Added _START_ and  _END_ labels to articles')

    pre = pd.DataFrame({'text': text, 'summary': summary})
    post_pre = pre.copy()

    # Add sostok and eostok
    post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \
        + ' eostok')
    print('Added sostok and eostok labels to summaries')

    # Save and return
    post_pre.to_csv(save_path, index=False)
    return post_pre


def save_tokenizer(tokenizer, path):
    """Saves a tokenizer to a given path"""
    with open(path, 'wb') as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)


def load_tokenizer(path):
     """Loads a tokenizer"""
     with open(path, 'rb') as handle:
        tokenizer = pickle.load(handle)
     return tokenizer


def train_model(post_pre, model_path, test_size=0.1, max_text_len=1700, max_summary_len=150):
   """
   Trains a model and saves it, input data is the initial dataframe and the path to save the model, rest of params can be changed but not necessarily, watch out max lengths
   """
   try:
      os.mkdir(model_path)
      os.mkdir(model_path + '/tokenizers')
   except:
      pass

   x_tr, x_val, y_tr, y_val = train_test_split(
       np.array(post_pre["text"]),
       np.array(post_pre["summary"]),
       test_size=test_size,
       random_state=0,
       shuffle=True,
   )

   # Tokenize the text to get the vocab count
   # Prepare a tokenizer on training data
   x_tokenizer = Tokenizer()
   x_tokenizer.fit_on_texts(list(x_tr))

   thresh = 5
   cnt = 0
   tot_cnt = 0

   for key, value in x_tokenizer.word_counts.items():
       tot_cnt = tot_cnt + 1
       if value < thresh:
           cnt = cnt + 1

   print("% of rare words in vocabulary: ", (cnt / tot_cnt) * 100)

   # Prepare a tokenizer, again -- by not considering the rare words
   x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) 
   x_tokenizer.fit_on_texts(list(x_tr))

   # Convert text sequences to integer sequences 
   x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) 
   x_val_seq = x_tokenizer.texts_to_sequences(x_val)

   # Pad zero upto maximum length
   x_tr = pad_sequences(x_tr_seq,  padding='post', maxlen=max_text_len)
   x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')

   # Size of vocabulary (+1 for padding token)
   x_voc = x_tokenizer.num_words + 1

   print("Size of vocabulary in X = {}".format(x_voc))

   # Prepare a tokenizer on testing data
   y_tokenizer = Tokenizer()
   y_tokenizer.fit_on_texts(list(y_tr))

   thresh = 5
   cnt = 0
   tot_cnt = 0

   for key, value in y_tokenizer.word_counts.items():
       tot_cnt = tot_cnt + 1
       if value < thresh:
           cnt = cnt + 1

   print("% of rare words in vocabulary:",(cnt / tot_cnt) * 100)

   # Prepare a tokenizer, again -- by not considering the rare words
   y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) 
   y_tokenizer.fit_on_texts(list(y_tr))

   # Convert text sequences to integer sequences 
   y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) 
   y_val_seq = y_tokenizer.texts_to_sequences(y_val) 

   # Pad zero upto maximum length
   y_tr = pad_sequences(y_tr_seq,  maxlen=max_summary_len, padding='post')
   y_val = pad_sequences(y_val_seq,  maxlen=max_summary_len, padding='post')

   # Size of vocabulary (+1 for padding token)
   y_voc = y_tokenizer.num_words + 1
   print("Size of vocabulary in Y = {}".format(y_voc))

   # Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens
   ind = []

   for i in range(len(y_tr)):
       cnt = 0
       for j in y_tr[i]:
           if j != 0:
               cnt = cnt + 1
       if cnt == 2:
           ind.append(i)

   y_tr = np.delete(y_tr, ind, axis=0)
   x_tr = np.delete(x_tr, ind, axis=0)

   # Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens
   ind = []
   for i in range(len(y_val)):
       cnt = 0
       for j in y_val[i]:
           if j != 0:
               cnt = cnt + 1
       if cnt == 2:
           ind.append(i)

   y_val = np.delete(y_val, ind, axis=0)
   x_val = np.delete(x_val, ind, axis=0)

   # Save tokenizers
   save_tokenizer(x_tokenizer, model_path + '/tokenizers/x_tokenizer.pickle')
   save_tokenizer(y_tokenizer, model_path + '/tokenizers/y_tokenizer.pickle')

   # Model
   print('Model Definition')

   # Encoder
   encoder_inputs = Input(shape=(max_text_len, ))

   # Embedding layer
   enc_emb = Embedding(x_voc, embedding_dim,
                       trainable=True)(encoder_inputs)

   # Encoder LSTM 1
   encoder_lstm1 = LSTM(latent_dim, return_sequences=True,
                        return_state=True, dropout=0.4,
                        recurrent_dropout=0.4)
   (encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)

   # Encoder LSTM 2
   encoder_lstm2 = LSTM(latent_dim, return_sequences=True,
                        return_state=True, dropout=0.4,
                        recurrent_dropout=0.4)
   (encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)

   # Encoder LSTM 3
   encoder_lstm3 = LSTM(latent_dim, return_state=True,
                        return_sequences=True, dropout=0.4,
                        recurrent_dropout=0.4)
   (encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)

   # Set up the decoder, using encoder_states as the initial state
   decoder_inputs = Input(shape=(None, ))

   # Embedding layer
   dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)
   dec_emb = dec_emb_layer(decoder_inputs)

   # Decoder LSTM
   decoder_lstm = LSTM(latent_dim, return_sequences=True,
                       return_state=True, dropout=0.4,
                       recurrent_dropout=0.2)
   (decoder_outputs, decoder_fwd_state, decoder_back_state) = \
        decoder_lstm(dec_emb, initial_state=[state_h, state_c])

   # Dense layer
   decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))
   decoder_outputs = decoder_dense(decoder_outputs)

   # Define the model
   model = Model([encoder_inputs, decoder_inputs], decoder_outputs)


   """
   # SIMPLER ARCHITECTURE
   # Encoder
   encoder_inputs = Input(shape=(max_text_len,))
   enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)
   encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)
   encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)

   # Decoder
   decoder_inputs = Input(shape=(None,))
   dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)
   dec_emb = dec_emb_layer(decoder_inputs)
   decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)
   decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])
   decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))
   decoder_outputs = decoder_dense(decoder_outputs)
   """
   # Define the model
   model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
   print(model.summary())
   model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', run_eagerly=True)
   es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)

   history = model.fit(
       [x_tr, y_tr[:, :-1]],
       y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],
       epochs=2,
       callbacks=[es],
       batch_size=128,
       validation_data=([x_val, y_val[:, :-1]],
                        y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:
                        , 1:]),
    )
   model.save(model_path + '/model.h5')
   return model, x_tokenizer, y_tokenizer


def load_model(path):
    """Returns a tuple with the model and the tokenizers -> model, x_tokenizer, y_tokenizer"""
    return load_model(path + 'model.h5'), load_tokenizer(path + '/tokenizers/x_tokenizer.pickle'), load_tokenizer(path + '/tokenizers/y_tokenizer.pickle')



pre = pd.read_csv('../data/cnn_dailymail/train.csv').sample(samples)
pre.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)
print(pre.head())


post_pre = prepare_data(pre)
model, x_tokenizer, y_tokenizer = train_model(post_pre, model_path)







reverse_target_word_index = y_tokenizer.index_word
reverse_source_word_index = x_tokenizer.index_word
target_word_index = y_tokenizer.word_index

# Inference Models
# Encode the input sequence to get the feature vector
encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,
                      state_h, state_c])

# Decoder setup
# Below tensors will hold the states of the previous time step
decoder_state_input_h = Input(shape=(latent_dim, ))
decoder_state_input_c = Input(shape=(latent_dim, ))
decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))

# Get the embeddings of the decoder sequence
dec_emb2 = dec_emb_layer(decoder_inputs)

# To predict the next word in the sequence, set the initial states to the states from the previous time step
(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,
        initial_state=[decoder_state_input_h, decoder_state_input_c])

# A dense softmax layer to generate prob dist. over the target vocabulary
decoder_outputs2 = decoder_dense(decoder_outputs2)

# Final decoder model
decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,
                      decoder_state_input_h, decoder_state_input_c],
                      [decoder_outputs2] + [state_h2, state_c2])

def decode_sequence(input_seq):

    # Encode the input as state vectors.
    (e_out, e_h, e_c) = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1
    target_seq = np.zeros((1, 1))

    # Populate the first word of target sequence with the start word.
    target_seq[0, 0] = target_word_index['sostok']

    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
        (output_tokens, h, c) = decoder_model.predict([target_seq]
                + [e_out, e_h, e_c])

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_token = reverse_target_word_index[sampled_token_index]

        if sampled_token != 'eostok':
            decoded_sentence += ' ' + sampled_token

        # Exit condition: either hit max length or find the stop word.
        if sampled_token == 'eostok' or len(decoded_sentence.split()) \
            >= max_summary_len - 1:
            stop_condition = True

        # Update the target sequence (of length 1)
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        # Update internal states
        (e_h, e_c) = (h, c)

    return decoded_sentence

# To convert sequence to summary
def seq2summary(input_seq):
    newString = ''
    for i in input_seq:
        if i != 0 and i != target_word_index['sostok'] and i \
            != target_word_index['eostok']:
            newString = newString + reverse_target_word_index[i] + ' '

    return newString


# To convert sequence to text
def seq2text(input_seq):
    newString = ''
    for i in input_seq:
        if i != 0:
            newString = newString + reverse_source_word_index[i] + ' '

    return newString

for i in range(0, 19):
    print ('Review:', seq2text(x_tr[i]))
    print ('Original summary:', seq2summary(y_tr[i]))
    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1, max_text_len)))
    print ('\n')
